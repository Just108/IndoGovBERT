{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This Program is to clean indonesian documents "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil\n",
    "import re\n",
    "import spacy\n",
    "import nltk\n",
    "import statistics\n",
    "\n",
    "from spacy.language import Language\n",
    "from langdetect import detect, detect_langs\n",
    "from spacy_langdetect import LanguageDetector\n",
    "from spacy_language_detection import LanguageDetector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_lang_detector(nlp, name):\n",
    "    return LanguageDetector(seed=42)  # We use the seed 42"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "directory = './test/txts/' #here are the original txt\n",
    "save_dir = './test/txts/hasil/' #the outputs will be generated here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "txt_filenames = [filename for filename in os.listdir(directory) if filename.endswith('.txt')]\n",
    "txt_filenames.sort()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "txt_filenames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "list_of_chars = ['|']\n",
    "pattern = '[' +  ''.join(list_of_chars) +  ']'\n",
    "\n",
    "for filename in saved_filenames:\n",
    "    filepath = os.path.join(directory, filename) \n",
    "    with open(filepath, 'r') as f:\n",
    "        lines = []\n",
    "        for line in f:\n",
    "            line = line.strip()\n",
    "            try:\n",
    "                #lang = detect(line) \n",
    "                #if lang == 'id':\n",
    "                lines.append(line) \n",
    "            except Exception:\n",
    "                pass\n",
    "                                           \n",
    "        print(lines)\n",
    "        \n",
    "\n",
    "                            words = nltk.tokenize.word_tokenize(line)\n",
    "                            #print(words)\n",
    "                            add=0\n",
    "                            for word in words:\n",
    "                                word_langs = detect_langs(word)\n",
    "                                print(word_langs)\n",
    "                                for word_language in word_langs:\n",
    "                                    if (word_language.lang == \"id\") or ((word_language.lang == \"en\") and (word_language.prob>0.7)):\n",
    "                                        add=1\n",
    "                                        continue\n",
    "                                    \n",
    "                            \n",
    "                            if add==1:\n",
    "                                lines.append(line) \n",
    "                                continue\n",
    "                            else:\n",
    "                                print(line) \n",
    "                                #print(word_langs) \n",
    "                                #pass    \n",
    "                                ,,,  \n",
    "        \n",
    "                \n",
    "                    doc = nlp_model(line) #3\n",
    "                    detect_language = doc._.language #4\n",
    "                    print(line, detect_language)\n",
    "                    '''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ignores words containing alphabet characters with a length less than 2, unless they contain a number or are in list:\n",
    "'''\n",
    "def ignore_small_words(text):\n",
    "    words = text.split()\n",
    "    filtered_words = [word for word in words if len(word) >= 3 or any(char.isdigit() for char in word) or word.lower() in list_3_words]\n",
    "    filtered_text = \" \".join(filtered_words)\n",
    "    return filtered_text\n",
    "    '''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#receives a text input and ignores the first or last word if it has a length less than 2:\n",
    "def ignore_small_words_first_last(text):\n",
    "    words = text.split()\n",
    "    if (len(words) > 0 and (len(words[0]) <= 2 and (words[0] not in [\".\",\",\"]))):\n",
    "        words = words[1:]\n",
    "    if (len(words) > 0 and (len(words[-1]) <= 2 and (words[-1] not in [\".\",\",\"]))):\n",
    "        words = words[:-1]\n",
    "    filtered_text = \" \".join(words)\n",
    "    return filtered_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The characters to search for and change to nothing\n",
    "# Nyari karakter2 yang dihapus\n",
    "list_of_chars = ['|','\\xa0','£','»']\n",
    "# Create the regular expression pattern by joining the search characters with the \"or\" operator\n",
    "pattern = '[' +  ''.join(list_of_chars) +  ']'\n",
    "\n",
    "# The strings to search for and then skip\n",
    "# Detect paging\n",
    "search_strings = [\n",
    "                  \"KONVENSI NASIONAL IAP - CIDES\",\n",
    "                 ]\n",
    "# Create the regular expression pattern by joining the search strings with the \"or\" operator\n",
    "pattern_strings = \"|\".join(search_strings)\n",
    "\n",
    "nlp_model = spacy.load(\"en_core_web_sm\")\n",
    "Language.factory(\"language_detector\", func=get_lang_detector)\n",
    "nlp_model.add_pipe('language_detector', last=True)\n",
    "\n",
    "\n",
    "for filename in txt_filenames:\n",
    "    filepath = os.path.join(directory, filename) \n",
    "    print(filepath)\n",
    "\n",
    "    with open(filepath, 'r') as f:\n",
    "        lines = []\n",
    "        for line in f:\n",
    "            #Ngapus character aneh\n",
    "            line = re.sub(pattern, '', line.strip())\n",
    "            #Ngapus kalau ada spasi berlebih\n",
    "            line = re.sub(' +', ' ', line)\n",
    "            #Ngapus kata pendek di depan dan belakang\n",
    "            line = ignore_small_words_first_last(line)\n",
    "            #Ngapus kata pendek\n",
    "            #line = ignore_small_words(line)\n",
    "            \n",
    "            # Use the `search` function from the `re` module to find the search string in the text\n",
    "            matches = re.findall(pattern_strings, line)\n",
    "            if len(matches) > 0:\n",
    "                continue\n",
    "            else:\n",
    "                try:\n",
    "                    \n",
    "                    langs = detect_langs(line)\n",
    "                    add=0\n",
    "                    for language in langs:\n",
    "                        if (language.lang == \"id\") :\n",
    "                            lines.append(line)\n",
    "                            break\n",
    "                        elif language.prob>0.4:\n",
    "                            words = nltk.tokenize.word_tokenize(line)\n",
    "                            average_length = statistics.mean(map(len,words))\n",
    "                            if (average_length<3):\n",
    "                                break\n",
    "                            else:   \n",
    "                                lines.append(line)\n",
    "                                break\n",
    "                        else: #Non Indonesian                                \n",
    "                            print(line,langs)\n",
    "                            continue\n",
    "\n",
    "                except Exception:\n",
    "                    pass\n",
    "        \n",
    "        occurrence = {item: lines.count(item) for item in lines}        \n",
    "        occurrence = {key:val for key, val in occurrence.items() if val <=2}\n",
    "\n",
    "        #print(occurrence)\n",
    "        text = (\" \".join(occurrence))\n",
    "        text = text.replace(\" . \", \" \")\n",
    "        text = re.sub(r'\\.+', '.', text)\n",
    "        with open(os.path.join(save_dir, filename), 'w') as f:\n",
    "            f.write(text)\n",
    "            del(text)\n",
    "            f.close()\n",
    "        #print(lines)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.4 ('dgl': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "1f51b1903ede40426e167c10e8ccc7e923a9d2193895121d7daf8840aef057af"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
