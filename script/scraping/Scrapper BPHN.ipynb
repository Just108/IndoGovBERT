{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Scraper for BPHN\n",
    "# 1. Peraturan DPRD\n",
    "# 2. Peraturan Bersama\n",
    "# 3. MOU\n",
    "# 4. PKS\n",
    "# 5. Peraturan PT\n",
    "# 6. Keputusan PT\n",
    "# 7. SE PT\n",
    "# 8. Kerjasama PT\n",
    "# 9. Intl Agreement\n",
    "# 10. Kolonial\n",
    "# 11. Monografi\n",
    "# 12. Artikel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_json('data_jdihn.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import json\n",
    "from bs4 import BeautifulSoup as bs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import numpy as np\n",
    "import math\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "from concurrent.futures import as_completed\n",
    "import urllib3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class peraturan: \n",
    "    def __init__(self, source, jenis, judul, link): \n",
    "        self.source = source \n",
    "        self.jenis = jenis\n",
    "        self.judul = judul \n",
    "        self.link = link"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extractPage(s,headers,jenis,token,page):\n",
    "    payload = {}\n",
    "    payload['jenis'] = jenis\n",
    "    payload['_token'] = token\n",
    "    payload['page'] = page\n",
    "    print(payload)\n",
    "    try:\n",
    "        r = s.post('https://jdihn.go.id/pencarian', headers=headers, params=payload)\n",
    "        soup = bs(r.content, 'html.parser')\n",
    "        \n",
    "        data_peraturan = []\n",
    "        lists = soup.find_all('div',{'class':'row align-items-md-center align-items-lg-stretch'})\n",
    "        for list in lists:\n",
    "            div_source = list.find(\"a\",{'class': 'media-title'})\n",
    "            div_jenis = list.find(\"span\",{'class': 'content-view'})\n",
    "            div_judul = list.find(\"a\",{'class': 'media-title text-capitalize'})\n",
    "            div_links = list.findAll(\"a\")\n",
    "            print(div_judul.text)\n",
    "            data_peraturan.append(peraturan(div_source.text,div_jenis.text,div_judul.text,div_links[2][\"href\"]))\n",
    "        return data_peraturan\n",
    "    except urllib3.exceptions.ProtocolError:\n",
    "        print(\"error!!\")\n",
    "        return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "headers = {'User-Agent': \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/98.0.4758.102 Safari/537.36\"}\n",
    "page_size = 8\n",
    "\n",
    "with requests.Session() as s:\n",
    "    r = s.get('https://jdihn.go.id/dokumen-hukum', headers=headers)\n",
    "    \n",
    "    soup = bs(r.content, 'html.parser')\n",
    "    token = soup.find('input',{'name':'_token'})\n",
    "    \n",
    "    data_peraturan_all = []\n",
    "    with ThreadPoolExecutor(max_workers=20) as exe:\n",
    "                 \n",
    "        for index, row in df.iterrows():\n",
    "            page_total = int(row[\"jumlah\"])\n",
    "\n",
    "            est_page = math.ceil(page_total/page_size)\n",
    "            print(row[\"name\"])\n",
    "            pages = np.arange(1, 1+est_page , 1)\n",
    "            print(pages)\n",
    "\n",
    "            futures = [exe.submit(extractPage, s, r.headers, row[\"name\"], token[\"value\"], page) for page in pages]       \n",
    "\n",
    "            for future in as_completed(futures):\n",
    "               data_peraturan_all.append(future.result())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(data_peraturan_all)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "data_peraturan_bphn= list(itertools.chain.from_iterable(data_peraturan_all))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(data_peraturan_all)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_peraturan_bphn[0].link"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lst = [[x.source, x.jenis, x.judul, x.link] for x in data_peraturan_bphn]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(lst,columns=['source','jenis','judul','link'])\n",
    "df.to_csv('bphn.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.7 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "7a3b7509ebdcf43811d3d9c95007d85565ecdcb3e16afeda183944cc0455e408"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
